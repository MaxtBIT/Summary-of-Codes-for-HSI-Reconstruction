import torch
import torch.nn as nn
from torch.nn import init
import torch.nn.functional as F
import scipy.io as sio
import numpy as np
import os
from torch.utils.data import Dataset, DataLoader
import platform
from argparse import ArgumentParser

parser = ArgumentParser(description='ISTA-Net')

parser.add_argument('--start_epoch', type=int, default=0, help='epoch number of start training')
parser.add_argument('--end_epoch', type=int, default=200, help='epoch number of end training')
parser.add_argument('--layer_num', type=int, default=9, help='phase number of ISTA-Net')
parser.add_argument('--learning_rate', type=float, default=1e-4, help='learning rate')
parser.add_argument('--group_num', type=int, default=1, help='group number for training')
parser.add_argument('--cs_ratio', type=int, default=25, help='from {1, 4, 10, 25, 40, 50}')
parser.add_argument('--gpu_list', type=str, default='0', help='gpu index')

parser.add_argument('--matrix_dir', type=str, default='sampling_matrix', help='sampling matrix directory')
parser.add_argument('--model_dir', type=str, default='model', help='trained or pre-trained model directory')
parser.add_argument('--data_dir', type=str, default='data', help='training data directory')
parser.add_argument('--log_dir', type=str, default='log', help='log directory')

args = parser.parse_args()


# start_epoch = args.start_epoch
# end_epoch = args.end_epoch
# learning_rate = args.learning_rate
# layer_num = args.layer_num
# group_num = args.group_num
# cs_ratio = args.cs_ratio
# gpu_list = args.gpu_list


# try:
#     # The flag below controls whether to allow TF32 on matmul. This flag defaults to True.
#     torch.backends.cuda.matmul.allow_tf32 = False
#     # The flag below controls whether to allow TF32 on cuDNN. This flag defaults to True.
#     torch.backends.cudnn.allow_tf32 = False
# except:
#     pass


# os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
# os.environ["CUDA_VISIBLE_DEVICES"] = gpu_list
# device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")


# ratio_dict = {1: 10, 4: 43, 10: 109, 25: 272, 30: 327, 40: 436, 50: 545}

# n_input = ratio_dict[cs_ratio]
# n_output = 1089
# nrtrain = 88912   # number of training blocks
# batch_size = 64


# # Load CS Sampling Matrix: phi
# Phi_data_Name = './models/ISTA_Net/phi_0_%d_1089.mat' % (args.cs_ratio)
# Phi_data = sio.loadmat(Phi_data_Name)
# Phi_input = Phi_data['phi']


# #Training_data_Name = 'Training_Data.mat'
# Training_data = sio.loadmat('./%s/%s' % (args.data_dir, Training_data_Name))
# Training_labels = Training_data['labels']


# Qinit_Name = './%s/Initialization_Matrix_%d.mat' % (args.matrix_dir, cs_ratio)


# # Computing Initialization Matrix:
# if os.path.exists(Qinit_Name):
#     Qinit_data = sio.loadmat(Qinit_Name)
#     Qinit = Qinit_data['Qinit']

# else:
#     X_data = Training_labels.transpose()
#     Y_data = np.dot(Phi_input, X_data)
#     Y_YT = np.dot(Y_data, Y_data.transpose())
#     X_YT = np.dot(X_data, Y_data.transpose())
#     Qinit = np.dot(X_YT, np.linalg.inv(Y_YT))
#     del X_data, Y_data, X_YT, Y_YT
#     sio.savemat(Qinit_Name, {'Qinit': Qinit})


# Define ISTA-Net Block
class BasicBlock(torch.nn.Module):
    def __init__(self):
        super(BasicBlock, self).__init__()

        self.lambda_step = nn.Parameter(torch.Tensor([0.5]))
        self.soft_thr = nn.Parameter(torch.Tensor([0.01]))

        self.conv1_forward = nn.Parameter(init.xavier_normal_(torch.Tensor(32, 1, 3, 3)))
        self.conv2_forward = nn.Parameter(init.xavier_normal_(torch.Tensor(32, 32, 3, 3)))
        self.conv1_backward = nn.Parameter(init.xavier_normal_(torch.Tensor(32, 32, 3, 3)))
        self.conv2_backward = nn.Parameter(init.xavier_normal_(torch.Tensor(1, 32, 3, 3)))

    def forward(self, x, PhiTPhi, PhiTb):
        x = x.view(33, 33)
        x = x - self.lambda_step * torch.mm(x, PhiTPhi)
        x = x + self.lambda_step * PhiTb
        x_input = x.view(-1, 1, 33, 33)
        # x_input = x.view(-1, 1, 33, 33)

        x = F.conv2d(x_input, self.conv1_forward, padding=1)
        x = F.relu(x)
        x_forward = F.conv2d(x, self.conv2_forward, padding=1)

        x = torch.mul(torch.sign(x_forward), F.relu(torch.abs(x_forward) - self.soft_thr))

        x = F.conv2d(x, self.conv1_backward, padding=1)
        x = F.relu(x)
        x_backward = F.conv2d(x, self.conv2_backward, padding=1)

        x_pred = x_backward.view(-1, 1089)

        x = F.conv2d(x_forward, self.conv1_backward, padding=1)
        x = F.relu(x)
        x_est = F.conv2d(x, self.conv2_backward, padding=1)
        symloss = x_est - x_input

        return [x_pred, symloss]


# Define ISTA-Net
class ISTANet(torch.nn.Module):
    def __init__(self, LayerNo):
        super(ISTANet, self).__init__()
        onelayer = []
        self.LayerNo = LayerNo

        for i in range(LayerNo):
            onelayer.append(BasicBlock())

        self.fcs = nn.ModuleList(onelayer)

    def forward(self, Phix):

        # only to calculate the parameters and FLOPs, modified by MaxtBIT
        Phix = Phix.squeeze(0)
        Phi = torch.Tensor([1.0]).unsqueeze(1)
        Phi = Phi.expand([33, 33]).cuda()
        Qinit  = torch.Tensor([1.0]).unsqueeze(1)
        Qinit = Qinit.expand([33, 33]).cuda()

        PhiTPhi = torch.mm(torch.transpose(Phi, 0, 1), Phi)
        PhiTb = torch.mm(Phix, Phi)

        x = torch.mm(Phix, torch.transpose(Qinit, 0, 1))

        layers_sym = []   # for computing symmetric loss

        for i in range(self.LayerNo):
            [x, layer_sym] = self.fcs[i](x, PhiTPhi, PhiTb)
            layers_sym.append(layer_sym)

        x_final = x

        return [x_final, layers_sym]


# model = ISTANet(layer_num)
# model = nn.DataParallel(model)
# model = model.to(device)



# print_flag = 1   # print parameter number

# if print_flag:
#     num_count = 0
#     for para in model.parameters():
#         num_count += 1
#         print('Layer %d' % num_count)
#         print(para.size())



# class RandomDataset(Dataset):
#     def __init__(self, data, length):
#         self.data = data
#         self.len = length

#     def __getitem__(self, index):
#         return torch.Tensor(self.data[index, :]).float()

#     def __len__(self):
#         return self.len


# if (platform.system() =="Windows"):
#     rand_loader = DataLoader(dataset=RandomDataset(Training_labels, nrtrain), batch_size=batch_size, num_workers=0,
#                              shuffle=True)
# else:
#     rand_loader = DataLoader(dataset=RandomDataset(Training_labels, nrtrain), batch_size=batch_size, num_workers=4,
#                              shuffle=True)

# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

# model_dir = "./%s/CS_ISTA_Net_layer_%d_group_%d_ratio_%d_lr_%.4f" % (args.model_dir, layer_num, group_num, cs_ratio, learning_rate)

# log_file_name = "./%s/Log_CS_ISTA_Net_layer_%d_group_%d_ratio_%d_lr_%.4f.txt" % (args.log_dir, layer_num, group_num, cs_ratio, learning_rate)

# if not os.path.exists(model_dir):
#     os.makedirs(model_dir)


# if start_epoch > 0:
#     pre_model_dir = model_dir
#     model.load_state_dict(torch.load('./%s/net_params_%d.pkl' % (pre_model_dir, start_epoch)))


# Phi = torch.from_numpy(Phi_input).type(torch.FloatTensor)
# Phi = Phi.to(device)

# Qinit = torch.from_numpy(Qinit).type(torch.FloatTensor)
# Qinit = Qinit.to(device)


# # Training loop
# for epoch_i in range(start_epoch+1, end_epoch+1):
#     for data in rand_loader:

#         batch_x = data
#         batch_x = batch_x.to(device)

#         Phix = torch.mm(batch_x, torch.transpose(Phi, 0, 1))

#         [x_output, loss_layers_sym] = model(Phix, Phi, Qinit)

#         # Compute and print loss
#         loss_discrepancy = torch.mean(torch.pow(x_output - batch_x, 2))

#         loss_constraint = torch.mean(torch.pow(loss_layers_sym[0], 2))
#         for k in range(layer_num-1):
#             loss_constraint += torch.mean(torch.pow(loss_layers_sym[k+1], 2))

#         gamma = torch.Tensor([0.01]).to(device)

#         # loss_all = loss_discrepancy
#         loss_all = loss_discrepancy + torch.mul(gamma, loss_constraint)

#         # Zero gradients, perform a backward pass, and update the weights.
#         optimizer.zero_grad()
#         loss_all.backward()
#         optimizer.step()

#         output_data = "[%02d/%02d] Total Loss: %.4f, Discrepancy Loss: %.4f,  Constraint Loss: %.4f\n" % (epoch_i, end_epoch, loss_all.item(), loss_discrepancy.item(), loss_constraint)
#         print(output_data)

#     output_file = open(log_file_name, 'a')
#     output_file.write(output_data)
#     output_file.close()

#     if epoch_i % 5 == 0:
#         torch.save(model.state_dict(), "./%s/net_params_%d.pkl" % (model_dir, epoch_i))  # save only the parameters
